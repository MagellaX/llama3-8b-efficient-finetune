{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":24927,"status":"ok","timestamp":1732284105449,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"},"user_tz":-330},"id":"FdOXrpS9w5ux"},"outputs":[],"source":["%%capture\n","!pip install unsloth\n","# Also get the latest nightly Unsloth!\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"]},{"cell_type":"markdown","metadata":{"id":"8OM2x8axxyYb"},"source":["model_setup.py"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67918,"status":"ok","timestamp":1732284173361,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"},"user_tz":-330},"id":"ImFx8sGSxEta","outputId":"7860a485-d0c6-4846-9a96-4e9f989d28b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2024.11.9: Fast Llama patching. Transformers = 4.46.2.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","GPU: Tesla T4\n","Max memory: 14.748 GB\n","Platform: linux\n","PyTorch: 2.5.1+cu121\n","CUDA: 12.1\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","import sys\n","from datasets import load_dataset\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","max_seq_length = 2048  # As shown in image\n","dtype = None  # Auto-detection\n","load_in_4bit = True\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n","    max_seq_length=max_seq_length,\n","    dtype=dtype,\n","    load_in_4bit=load_in_4bit,\n",")\n","\n","print(f\"GPU: Tesla T4\")\n","print(f\"Max memory: {torch.cuda.get_device_properties(0).total_memory/1024**3:.3f} GB\")\n","print(f\"Platform: {sys.platform}\")\n","print(f\"PyTorch: {torch.__version__}\")\n","print(f\"CUDA: {torch.version.cuda}\")"]},{"cell_type":"markdown","metadata":{"id":"ocEyMbtmyBtg"},"source":["LoRA_setup.py"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9nfYAZPxPFC","executionInfo":{"status":"ok","timestamp":1732284184226,"user_tz":-330,"elapsed":10871,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"ddf7e3dd-6c7f-4802-e3ee-33b04c081166"},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2024.11.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]}],"source":["# We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules=[\n","        \"q_proj\",   # Query projection\n","        \"k_proj\",   # Key projection\n","        \"v_proj\",   # Value projection\n","        \"o_proj\",   # Output projection\n","        \"gate_proj\",  # Gate projection\n","        \"up_proj\",    # Up projection\n","        \"down_proj\"   # Down projection\n","    ],\n","    lora_alpha=16,  # Same as r for balanced contribution\n","    lora_dropout=0,  # Supports any, but = 0 is optimized\n","    bias=\"none\",     # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n","    random_state=3407,\n","    use_rslora=False,  # We support rank stabilized LoRA\n","    loftq_config=None  # And LoFTQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"-06waHmRyK7e"},"source":["data_prep.py"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zs_pCwzOyJSx","executionInfo":{"status":"ok","timestamp":1732284189079,"user_tz":-330,"elapsed":4860,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"6aa51863-ccdb-4bac-e86a-a30dfd827cf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset prepared successfully!\n","Number of examples: 51760\n"]}],"source":["# Data Prep\n","# We now use the Alpaca dataset from yahma\n","dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n","\n","# Format prompts\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs = examples[\"input\"]\n","    outputs = examples[\"output\"]\n","    texts = []\n","\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","\n","    return { \"text\": texts }\n","\n","# Map dataset\n","dataset = dataset.map(\n","    formatting_prompts_func,\n","    batched=True,\n",")\n","\n","print(\"Dataset prepared successfully!\")\n","print(f\"Number of examples: {len(dataset)}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P32QpsdCyjFz","executionInfo":{"status":"ok","timestamp":1732284189659,"user_tz":-330,"elapsed":586,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"fe96eedc-f795-426a-9ee3-66e57a42f337"},"outputs":[{"output_type":"stream","name":"stderr","text":["max_steps is given, it will override any value given in num_train_epochs\n"]},{"output_type":"stream","name":"stdout","text":["\n","Starting training with the following setup:\n","Number of examples: 51760\n","Batch size per device: 2\n","Gradient accumulation steps: 4\n","Total batch size: 8\n","Number of steps: 60\n"]}],"source":["# Import and setup training\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","\n","\n","def check_bf16_support():\n","    return torch.cuda.is_bf16_supported()\n","\n","# Training setup without WandB\n","trainer = SFTTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    train_dataset=dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    dataset_num_proc=2,\n","    packing=False,\n","    args=TrainingArguments(\n","        per_device_train_batch_size=1,\n","        gradient_accumulation_steps=8,\n","        warmup_steps=5,\n","        max_steps=60,\n","        learning_rate=2e-4,\n","        fp16=not check_bf16_support(),\n","        bf16=check_bf16_support(),\n","        logging_steps=1,\n","        optim=\"adamw_8bit\",\n","        weight_decay=0.01,\n","        lr_scheduler_type=\"linear\",\n","        seed=3407,\n","        output_dir=\"outputs\",\n","        report_to=\"none\"  # Disable WandB logging\n","    )\n",")\n","\n","print(\"\\nStarting training with the following setup:\")\n","print(f\"Number of examples: {len(dataset)}\")\n","print(f\"Batch size per device: 2\")\n","print(f\"Gradient accumulation steps: 4\")\n","print(f\"Total batch size: {2 * 4}\")\n","print(f\"Number of steps: 60\")\n"]},{"cell_type":"markdown","metadata":{"id":"w4mq4mOhDgPe"},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3Ds4a_9RQNd","executionInfo":{"status":"ok","timestamp":1732284189659,"user_tz":-330,"elapsed":9,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"514b66b9-1425-4809-abb5-c05af9501780"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU = Tesla T4. Max memory = 14.748 GB.\n","6.004 GB of memory reserved.\n"]}],"source":["#@title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NSuDbyXCD4-A","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1732284769936,"user_tz":-330,"elapsed":580283,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"db71da99-312a-4cd9-c6c4-9d1520f4f773"},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n","   \\\\   /|    Num examples = 51,760 | Num Epochs = 1\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 8\n","\\        /    Total batch size = 8 | Total steps = 60\n"," \"-____-\"     Number of trainable parameters = 41,943,040\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [60/60 09:15, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.586600</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>2.114900</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.672700</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.862700</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.678700</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.489800</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.080600</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.269700</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.144000</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.122100</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.904300</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.997300</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.919100</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.079400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.900000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.863400</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.992000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.338200</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.033100</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.871900</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.923500</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.988400</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.990500</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.995400</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.072000</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.045800</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.040500</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.925900</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.927000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.852400</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.861300</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.873800</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>1.009100</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.862000</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.999300</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.879500</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.819500</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.733800</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>1.060800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.180600</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.901400</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.968100</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.878400</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.911600</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.968900</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.941300</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.816000</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>1.239800</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.843800</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.059200</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>1.038600</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.898600</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>1.006100</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>1.174600</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.794500</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.998800</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.880100</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.770900</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.837100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.885000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uOYTgTwPRYHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732284769936,"user_tz":-330,"elapsed":15,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"db10d00b-3e5f-430b-e6b6-70ac9478294e"},"outputs":[{"output_type":"stream","name":"stdout","text":["575.8494 seconds used for training.\n","9.6 minutes used for training.\n","Peak reserved memory = 7.371 GB.\n","Peak reserved memory for training = 1.367 GB.\n","Peak reserved memory % of max memory = 49.98 %.\n","Peak reserved memory for training % of max memory = 9.269 %.\n"]}],"source":["#@title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory         /max_memory*100, 3)\n","lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","metadata":{"id":"AqmurcOYIgOe"},"source":["INFERENCE"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"PP3oMPLvE6va","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732284775126,"user_tz":-330,"elapsed":5201,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"f53d6bfa-ee40-4ca4-9306-0abd3dc1370e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generated Response:\n","['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonacci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025']\n"]}],"source":["# Import required libraries\n","from unsloth import FastLanguageModel\n","import torch\n","\n","# Enable native 2x faster inference\n","model = FastLanguageModel.for_inference(model)\n","\n","# Setup inference\n","inputs = tokenizer(\n","    [\n","        alpaca_prompt.format(\n","            \"Continue the fibonacci sequence.\",  # instruction\n","            \"1, 1, 2, 3, 5, 8\",  # input\n","            \"\"  # output - leave blank for generation!\n","        )\n","    ], return_tensors = \"pt\").to(\"cuda\")\n","\n","# Generate with optimized settings\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens = 64,\n","    use_cache = True\n",")\n","\n","# Decode output\n","print(\"\\nGenerated Response:\")\n","print(tokenizer.batch_decode(outputs))"]},{"cell_type":"markdown","metadata":{"id":"h02VUckgImkX"},"source":["inference_test.py\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"o75tjTLSIkKC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732284805198,"user_tz":-330,"elapsed":30077,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"f7118823-ae81-48ff-d0df-ec57dec2715b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Unsloth: WARNING `trust_remote_code` is True.\n","Are you certain you want to do remote code execution?\n","==((====))==  Unsloth 2024.11.9: Fast Llama patching. Transformers = 4.46.2.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","\n","Testing with prompt: Continue the fibonacci sequence.\n","Input: 1, 1, 2, 3, 5, 8\n","\n","Generating response...\n","\n","Response:\n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Continue the fibonacci sequence.\n","\n","### Input:\n","1, 1, 2, 3, 5, 8\n","\n","### Response:\n","11, 18\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import TextStreamer\n","\n","def load_model(model_path=\"outputs\"):\n","    print(\"Loading model...\")\n","    # Load base model first\n","    base_model_name = \"unsloth/Meta-Llama-3.1-8B\"\n","\n","    # Initialize with base model configuration\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name=base_model_name,\n","        max_seq_length=2048,\n","        load_in_4bit=True,\n","        trust_remote_code=True\n","    )\n","\n","    # Enable faster inference\n","    model = FastLanguageModel.for_inference(model)\n","    return model, tokenizer\n","\n","def generate_response(model, tokenizer, instruction, input_text=\"\"):\n","    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{instruction}\n","\n","### Input:\n","{input_text}\n","\n","### Response:\n","\"\"\"\n","\n","    # Tokenize with proper padding\n","    inputs = tokenizer(\n","        [prompt],\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=2048\n","    ).to(\"cuda\")\n","\n","    # Generate with optimized settings\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=128,\n","        use_cache=True,\n","        temperature=0.7,\n","        top_p=0.95,\n","        pad_token_id=tokenizer.eos_token_id\n","    )\n","\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","if __name__ == \"__main__\":\n","    # Test the model\n","    model, tokenizer = load_model()\n","\n","    # Simple test case\n","    test_prompt = \"Continue the fibonacci sequence.\"\n","    test_input = \"1, 1, 2, 3, 5, 8\"\n","\n","    print(f\"\\nTesting with prompt: {test_prompt}\")\n","    print(f\"Input: {test_input}\")\n","    print(\"\\nGenerating response...\")\n","\n","    response = generate_response(model, tokenizer, test_prompt, test_input)\n","    print(\"\\nResponse:\")\n","    print(response)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"1LLkcPwTI4lP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732284807528,"user_tz":-330,"elapsed":2332,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"36ac085d-3cc3-4cf1-a82c-2bd538171d8f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13\\n\\n### Explanation:\\nThe next number in the fibonnaci sequence is the sum of the previous two. The previous two numbers are 5 and 8, so the next number is 5 + 8 = 13.\\n<|end_of_text|>']"]},"metadata":{},"execution_count":11}],"source":["# alpaca_prompt = Copied from above\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Continue the fibonnaci sequence.\", # instruction\n","        \"1, 1, 2, 3, 5, 8\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n","tokenizer.batch_decode(outputs)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"hAES9_c7SQ_G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732284808721,"user_tz":-330,"elapsed":1201,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"ca0ea298-d228-40a1-ad81-9b270f10870a"},"outputs":[{"output_type":"stream","name":"stdout","text":["<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Continue the fibonnaci sequence.\n","\n","### Input:\n","1, 1, 2, 3, 5, 8\n","\n","### Response:\n","13\n","<|end_of_text|>\n"]}],"source":["# alpaca_prompt = Copied from above\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"Continue the fibonnaci sequence.\", # instruction\n","        \"1, 1, 2, 3, 5, 8\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"J0fhUVG5ZuIR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732285057298,"user_tz":-330,"elapsed":248579,"user":{"displayName":"Jack Downer","userId":"06456391318318236429"}},"outputId":"f94bacaa-9b3d-4c5b-b262-b96e70ab5b68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["('lora_model/tokenizer_config.json',\n"," 'lora_model/special_tokens_map.json',\n"," 'lora_model/tokenizer.json')"]},"metadata":{},"execution_count":13}],"source":["model.save_pretrained(\"lora_model\") # Local saving\n","tokenizer.save_pretrained(\"lora_model\")\n","# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n","# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"]},{"cell_type":"code","source":[],"metadata":{"id":"XNRzz1rTQzPK"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyM8d8qRgtFyAeRSDqzuDxxW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}